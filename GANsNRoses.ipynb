{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>.container { width:90% !important; }\n",
       ".text_cell_render, .output_text {\n",
       "    font-family: Lato;\n",
       "    font-size: 18px;\n",
       "    line-height: 1.5;\n",
       "}\n",
       ".CodeMirror {\n",
       "    font-size: 16px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>.container { width:90% !important; }\n",
    ".text_cell_render, .output_text {\n",
    "    font-family: Lato;\n",
    "    font-size: 18px;\n",
    "    line-height: 1.5;\n",
    "}\n",
    ".CodeMirror {\n",
    "    font-size: 16px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs N' Roses: Understanding Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Me\n",
    "\n",
    "I taught machine learning and distributed computing technologies at Data Science Retreat, the longest-running Berlin-based bootcamp, for more than three years, helping more than 150 students advance their careers.\n",
    "\n",
    "My professional background includes 20 years of experience working for companies in several industries: banking, government, fintech, retail and mobility.\n",
    "\n",
    "I write regularly for Towards Data Science. My blog post [\"Understanding PyTorch with an example: a step-by-step tutorial\"](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e) reached more than 300,000 views since it was published.\n",
    "\n",
    "The positive feedback from the readers motivated me to write the series of book [\"Deep Learning with PyTorch Step-by-Step: A Beginner's Guide\"](https://pytorchstepbystep.com), which covers a broader range of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: DeepFakes, GANs, and Synthetic data\n",
    "\n",
    "Generative models are at the heart of DeepFakes, and can be used to synthesize, replace, or swap attributes of images.\n",
    "\n",
    "In this workshop, we will learn the basics of Generative Adversarial Networks, the famous GANs, from the ground up: latent spaces, autoencoders, generators, discriminators, GANs, DCGANs, WGANs, and more.\n",
    "\n",
    "The main goal of this sessions is to show you how GANs work: we will start with a simple example using synthetic data (not generated by GANs) to learn about latent spaces and how to use them to generate more synthetic data (using GANs to generate them). We will improve on the model's architecture, incorporating convolutional layers (DCGAN), different loss functions (WGAN, WGAN-GP) and use them to generate synthetic images of flowers (the roses!).\n",
    "\n",
    "We will use Google Colab and work our way together into building and training several GANs. You should be comfortable using Jupyter notebooks and Numpy, and training simple models in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What** are DeepFakes?\n",
    "\n",
    "---\n",
    "\n",
    "\"Deepfakes are synthetic media in which a person in an existing image or video is replaced with someone else's likeness.\" \n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Deepfake)\n",
    "\n",
    "---\n",
    "\n",
    "There are three main types of fakes:\n",
    "\n",
    "1. Face Synthesis (StyleGAN)\n",
    "   - https://www.thispersondoesnotexist.com/\n",
    "   - https://github.com/lucidrains/stylegan2-pytorch\n",
    "   - https://github.com/lucidrains/lightweight-gan\n",
    "   \n",
    "\n",
    "2. Face Swap (AutoEncoders)\n",
    "   - https://github.com/deepfakes/faceswap\n",
    "   - https://github.com/iperov/DeepFaceLive\n",
    "   \n",
    "\n",
    "3. Manipulating Face Attributes and Expressions (StarGAN)\n",
    "   - https://github.com/streamlit/demo-face-gan\n",
    "   \n",
    "The images below were generated using https://www.thispersondoesnotexist.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |  |  |  |  |\n",
    "|---|---|---|---|---|\n",
    "| ![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/tpdne1.jpeg) | ![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/tpdne2.jpeg) | ![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/tpdne3.jpeg) | ![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/tpdne4.jpeg) | ![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/tpdne5.jpeg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs\n",
    "\n",
    "The ability to generate synthetic images or manipulate existing ones has been greatly improved by one particular development: the rise of Generative Adversarial Networks. In its seminal paper, [\"Generative Adversarial Networks\"](https://arxiv.org/abs/1406.2661), Ian Goodfellow et al. introduced a clever, yet simple idea: pitting one neural network against the other!\n",
    "\n",
    "One of the networks plays the role of a **forger**, trying to fool the other network, which plays the role of an **appraiser** that's constantly looking for clues that a given image might be a forgery.\n",
    "\n",
    "So, as the forger gets better at generating believable fakes, the appraiser needs to up its game to keep up with it, and so on and so forth.\n",
    "\n",
    "Nowadays, there are *countless* types of GANs - they are versatile, and researches have proposed modifications to the original architecture to accomplish different tasks. While extremely versatile, GANs are also known to be somewhat *finicky* and hard to train.\n",
    "\n",
    "If you're curious about the different types of GANs out there, check the [GAN ZOO](https://github.com/hindupuravinash/the-gan-zoo) out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An MNIST-like Dataset of Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/gans.py --output gans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from PIL import Image\n",
    "from gans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(radius, center_x=0.5, center_y=0.5, size=28):\n",
    "    # draw a circle using coordinates for the center, and the radius\n",
    "    circle = plt.Circle((center_x, center_y), radius, color='k', fill=False)\n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.add_patch(circle)\n",
    "    ax.axis('off')\n",
    "    buf = fig.canvas.print_to_buffer()\n",
    "    plt.close()\n",
    "    # converts matplotlib figure into PIL image, make it grayscale, and resize it\n",
    "    return np.array(Image.frombuffer('RGBA', buf[1], buf[0]).convert('L').resize((int(size), int(size))))\n",
    "\n",
    "def gen_circles(n, size=28):\n",
    "    # generates random coordinates around (0.5, 0.5) as center points\n",
    "    center_x = np.random.uniform(0.0, 0.03, size=n).reshape(-1, 1)+.5\n",
    "    center_y = np.random.uniform(0.0, 0.03, size=n).reshape(-1, 1)+.5\n",
    "    # generates random radius sizes between 0.03 and 0.47\n",
    "    radius = np.random.uniform(0.03, 0.47, size=n).reshape(-1, 1)\n",
    "    sizes = np.ones((n, 1))*size\n",
    "\n",
    "    coords = np.concatenate([radius, center_x, center_y, sizes], axis=1)\n",
    "    # generates circles using draw_circle function\n",
    "    circles = np.apply_along_axis(func1d=lambda v: draw_circle(*v), axis=1, arr=coords)\n",
    "    return circles, radius\n",
    "\n",
    "np.random.seed(42)\n",
    "# generates 1,000 circles\n",
    "circles, radius = gen_circles(1000)\n",
    "\n",
    "circles_ds = TensorDataset((torch.as_tensor(circles).unsqueeze(1).float()/255-.5)/.5, torch.as_tensor(radius))\n",
    "circles_dl = DataLoader(circles_ds, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure1(circles_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Latent Spaces and AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space\n",
    "\n",
    "The latent space is a **vector** containing one value for each **dimension**. It is usually represented by the letter ***z***.\n",
    "\n",
    "**YOU** get to choose how many dimensions the latent space will have!\n",
    "\n",
    "![latent space](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/latent_space.png)\n",
    "\n",
    "Let's keep it simple: our latent space has a **single dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 1\n",
    "z = torch.randn((5, z_size)).float()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be thinking, \"*but **which** dimensions are that, what do they mean*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like **embeddings** in NLP, the learned representations in the latent space ***may*** have some meaning (this is not a guarantee, and we're the ones assigning *meaning* to a dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/embed_arithmetic.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space, however many dimensions it may have, will be **sampled**, and then we'll use a given sample to **build an image**.\n",
    "\n",
    "In other words, we'll sample from a known distribution (usually, standard Normal) for each and every dimension, just like this:\n",
    "\n",
    "```python\n",
    "torch.randn((1, z_size)).float()\n",
    "```\n",
    "\n",
    "**YES, it is random!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go from a random vector to full image? How about using a **neural network**?\n",
    "\n",
    "The **decoder** plays the role of **mapping the (normally-distributed) latent space (*z*)** to a completely different distribution, in this case, **images**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decoder](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/decoder_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's build a simple sequential model in PyTorch that takes a **vector *z*** and outputs an **image** in the CHW shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "    # z_size -> 2048\n",
    "    nn.Linear(z_size, 2048),\n",
    "    nn.LeakyReLU(),\n",
    "    # 2048 -> 2048\n",
    "    nn.Linear(2048, 2048),\n",
    "    nn.LeakyReLU(),\n",
    "    # 2048 -> C*H*W\n",
    "    nn.Linear(2048, np.prod(input_shape)),\n",
    "    # C*H*W -> (C, H, W)\n",
    "    nn.Unflatten(1, input_shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that the decoder is doing its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde = decoder(z)\n",
    "x_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[1], show(x_tilde[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah... it converted a point in our one-dimensional latent space into an image.\n",
    "\n",
    "Obviously, we don't need a decoder to generate images of *noise*, right?\n",
    "\n",
    "It boils down to one question: **how do we get proper, meaningful, latent spaces?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're using a **decoder** to map from latent space to images, why not use an **encoder** to do exactly the opposite, that is, **map an image into the latent space (*z*)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/encoder_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to actually build an encoder here, but an encoder could look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Encoder(\n",
    "  (base_model): Sequential(\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "    (1): Linear(in_features=784, out_features=2048, bias=True)\n",
    "    (2): LeakyReLU(negative_slope=0.01)\n",
    "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "    (4): LeakyReLU(negative_slope=0.01)\n",
    "  )\n",
    "  (lin_latent): Linear(in_features=2048, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what happens if we put the two of them together, an encoder and a decoder?\n",
    "\n",
    "That's an **autoencoder**! It works like this:\n",
    "\n",
    "- it takes **an image** as **input**\n",
    "- the **encoder** part **maps the image to a latent space (*z*)\n",
    "- the **latent space** is the **input of the decoder** part\n",
    "- the **decoder** part **maps the latent space (*z*) back to an image**\n",
    "\n",
    "The **goal** of the autoencoder is to **reconstruct the images**, so, the **generated image** is as close as possible to the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/ae_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # when encoder met decoder\n",
    "        enc_out = self.enc(x)\n",
    "        return self.dec(enc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's **so** much more to autoencoders! The **variational** flavor of autoencoders, for example, can be used to **generate data** that's not completely limited by the data available in the training set. \n",
    "\n",
    "But we're just using them as a example, since they represent **one way to tackle** our main question \"*how do we get proper, meaningful, latent spaces*\".\n",
    "\n",
    "Guess what the **other way** is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Your First GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks, GANs, provide a different, and quite interesting way to tackle the challenge of achieving proper, meaningful, latent spaces.\n",
    "\n",
    "Let's take a closer look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Random Noise + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start, once again, with a **random sample** from our latent space (*z*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![latent space](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/latent_space.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_sample(batch_size, z_size, mode_z='normal'):\n",
    "    if mode_z == 'uniform':\n",
    "        input_z = torch.rand(batch_size, z_size) * 2 - 1 # make it zero centered\n",
    "    elif mode_z == 'normal':\n",
    "        input_z = torch.randn(batch_size, z_size)\n",
    "    return input_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, though, let's use a **latent space with 20 dimensions**, and generate a **mini-batch of vectors sampled from that latent space**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 20\n",
    "\n",
    "torch.manual_seed(1)\n",
    "input_z = latent_sample(32, z_size, 'normal')\n",
    "input_z[0], input_z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the samples from the latent space to, once again, **generate images** using the Decoder, now known as the **Generator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator (former Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![generator](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first GANs were built using fully-connected layers, like our decoder. But, soon enough, DCGANs (Deep Convolutional GANs) became the norm. Nowadays, a GAN is assumed to be a DCGAN (if we're handling images, of course). \n",
    "\n",
    "DCGANs were introduced in the paper [\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"](https://arxiv.org/abs/1511.06434) by Radford A. et al.\n",
    "\n",
    "The paper prescribed some improvements on regular GANs:\n",
    " - Remove fully-connected layers\n",
    " - Use BatchNorm\n",
    " - Use Adam optimizer\n",
    " - Generator\n",
    "   - Use ReLU activation in the Generator for all layers, except the output layer (Tanh)\n",
    "   - Use transposed convolutions (they are like \"inverse convolutions\", we'll get to them soon!)\n",
    " - Discriminator (that's the **Classifier** from Solution 2's title)\n",
    "   - Use strided convolutions instead of pooling layers\n",
    "   - **Use LeakyReLU activation**\n",
    " \n",
    "For more details on this, check Jason Brownlee's post on [\"How to Train Stable Generative Adversarial Networks\"](https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/).\n",
    "\n",
    "By the way, don't take the list above too literally - we'll see that some of them do not hold anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "\n",
    "z_size= 20\n",
    "in_channels = 1\n",
    "n_filters = 32\n",
    "\n",
    "# z_size as channels of a single pixel!\n",
    "generator = nn.Sequential(\n",
    "    # z_size -> z_size@1x1\n",
    "    nn.Unflatten(1, (z_size, 1, 1)),\n",
    "    \n",
    "    # z_size@1x1 -> 64@4x4\n",
    "    nn.ConvTranspose2d(z_size, n_filters*2, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@4x4 -> 64@7x7\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@7x7 -> 32@14x14\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 32@14x14 -> in_channels@28x28\n",
    "    nn.ConvTranspose2d(n_filters, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we feed our mini-batch of random samples from the latent space to the generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde = generator(input_z)\n",
    "x_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(x_tilde[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise, just as before, after all, it's all random! To address this, we're **NOT** using an encoder, but a **classifier**, the **Discriminator**, instead.\n",
    "\n",
    "Before moving on, though, let's take a quick look at the **unflatten** and **transposed convolution** layers in our **Generator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unflatten layer is transforming a **20-dimensional sample from the latent space** into a **20-channel single-pixel \"image\"**.\n",
    "\n",
    "Making it a \"pixel\" allows us to use **transposed convolution** to **increase height and width** of our single-pixel image all the way up to the desired image size (1@28x28)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/unflatten_z.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = torch.empty(5, z_size)\n",
    "unflattener = nn.Unflatten(1, (z_size, 1, 1))\n",
    "latent.shape, unflattener(latent).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - Stride = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposed convolutions are a bit weird... they all have their own **padding (transposed)**, **stride (transposed)** and **kernel size** arguments, but there are **implicit arguments** too!\n",
    "\n",
    "Let's start with the **implicit padding**, which is given by: `kernel size - padding(transposed) - 1`.\n",
    "\n",
    "It goes like this:\n",
    "- The original image gets **padded** with the **implicit padding**\n",
    "- A **regular convolution** is performed on the padded image using the kernel/filter and **stride of one**.\n",
    "\n",
    "Yeah, the **stride (transposed)** wasn't used at all! It will **only be used** if it is **greater than one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/conv_transp_s1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - Stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the **stride (transposed)** is greater than one, there's an **extra step** at the start: the input image gets **stuffed with zeros** by adding columns and rows full of zeroes **in-between the existing columns and rows**.\n",
    "\n",
    "The number of **added columns/rows** is given by: `stride (transposed) - 1`.\n",
    "\n",
    "BTW, the **implicit padding** is calculated just like before.\n",
    "\n",
    "It goes like this:\n",
    "- The original image gets **stuffed** with as many columns and rows full of zeros as given by the **stride (transposed) minus one**.\n",
    "- The stuffed image gets **padded** with the **implicit padding**\n",
    "- A **regular convolution** is performed on the padded image using the kernel/filter and **stride of one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/conv_transp_s2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's not clear yet, the code below should help you make sense of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input_image = torch.ones((1, 1, 2, 2)) # N, C, H, W\n",
    "input_image\n",
    "\n",
    "kernel_size = 2\n",
    "kernel = torch.ones((1, 1, kernel_size, kernel_size))\n",
    "\n",
    "stride_transp = 2\n",
    "padding_transp = 0\n",
    "\n",
    "F.conv_transpose2d(input_image,\n",
    "                   weight=kernel,\n",
    "                   stride=stride_transp,\n",
    "                   padding=padding_transp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_added_image = torch.tensor([[[[1, 0, 1],\n",
    "                                   [0, 0, 0],\n",
    "                                   [1, 0, 1]]]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_implicit = kernel_size - padding_transp - 1\n",
    "stride = 1\n",
    "\n",
    "F.conv2d(zero_added_image,\n",
    "         weight=kernel,\n",
    "         stride=1,\n",
    "         padding=padding_implicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - Increasing Dimensions of the Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to remeber is that the **transposed convolution** is used to **increase height and weight** (and even channels, like a regular convolution does) of the output image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{stride}^T \\times (\\text{image_size}-1) + \\text{kernel_size} - 2 \\times \\text{padding}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classifier (aka Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of an *encoder*, we'll be using a **classifier**, also known as, **Discriminator**.\n",
    "\n",
    "Its job is fairly straightforward: classify a given image as either **real (1) or fake (0)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![discriminator](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/discriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below takes a sequential model (`base_model`), figures its output size (assuming its flattened), and uses it as input to a **linear output layer with a sigmoid activation - a simple logistic regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, base_model):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.base_model = base_model\n",
    "\n",
    "        output_size = self._get_output_size()\n",
    "        # appends the \"lin_classifier\" linear layer to map from \"output_size\" \n",
    "        # given by the base model to the output of a binary classifier\n",
    "        output_size = self._get_output_size()\n",
    "        self.lin_classifier = nn.Linear(output_size, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def _get_output_size(self):\n",
    "        # builds a dummy batch containing one dummy tensor\n",
    "        # full of zeroes with the same shape as the inputs\n",
    "        device = next(self.base_model.parameters()).device.type\n",
    "        dummy = torch.zeros(1, *self.input_shape, device=device)\n",
    "        # sends the dummy batch through the base model to get \n",
    "        # the output size produced by it\n",
    "        size = self.base_model(dummy).size(1)\n",
    "        return size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forwards the input through the base model and then the \"lin_classifier\" layer \n",
    "        # to get the representation (z)\n",
    "        base_out = self.base_model(x)\n",
    "        logits = self.lin_classifier(base_out)\n",
    "        out = self.activation(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model itself is a typical **convolutional neural network**. It takes our circle images as inputs (1@28x28), convolves them several times until their dimensions are 64@7x7, and flattens them for the classifier part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "\n",
    "# we defined our representation (z) as a vector of size twenty\n",
    "z_size = 20\n",
    "in_channels = 1\n",
    "n_filters = 32\n",
    "# our images are 1@28x28\n",
    "input_shape = (1, 28, 28) # (C, H, W)\n",
    "\n",
    "disc_base = nn.Sequential(\n",
    "    # in_channels@28x28 -> 32@28x28\n",
    "    nn.Conv2d(in_channels, n_filters, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters),\n",
    "    nn.LeakyReLU(0.2),\n",
    "            \n",
    "    # 32@28x28 -> 64@14x14\n",
    "    nn.Conv2d(n_filters, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@14x14 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 3136\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "\n",
    "discriminator = Discriminator(input_shape, disc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a **symmetry** between **discriminator** and **generator**. We should try to build them with similar complexity/power, in order to create a level playing field for them to compete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN is simply a combination of Generator and Discriminator, like in the diagram below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gan](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare it to the architecture of the AutoEncoder below (and notice the colors I used). \n",
    "\n",
    "The **Decoder** and the **Generator** are performing the same operation, that is, **generating images**. \n",
    "\n",
    "Although the **Discriminator** and the **Encoder** are performing different operations, they share the **same architectural principle**, that is, being a **mirror-image** (of sorts) of their counterparts, respectively, the Generator and the Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/ae_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x_fake = self.generator(z)\n",
    "        prob = self.discriminator(x_fake)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(generator, discriminator)\n",
    "gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models themselves, Discriminator and Generator, are quite straightforward, there's nothing special about them.\n",
    "\n",
    "The **secret sauce** lies in the **training**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/jonathan-borba-YRg1q2qCq8A-unsplash.jpg\" />\n",
    "<p style=\"text-align:center\">\n",
    "Photo by <a href=\"https://unsplash.com/@jonathanborba?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Jonathan Borba</a> on <a href=\"https://unsplash.com/s/photos/sauce?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN\n",
    "\n",
    "It turns out, one **does not train the whole GAN at once**. We **take turns**, training each part, **Discriminator** and **Generator**, alternately.\n",
    "\n",
    "First, we train the Discriminator using **two batches** of images, **real** and **fake**, and their **true labels**.\n",
    "\n",
    "Then we train the Generator using **fake images**, but use **labels as if they were real** (or **pretend labels**, as I like to call them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch of Real Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/disc_train1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use a mini-batch of **real images** from our **data loader**, the usual way. \n",
    "\n",
    "We'll use them as inputs to the **Discriminator**, get the corresponding **probabilities**, and use **binary cross-entropy** as the **loss function**. \n",
    "\n",
    "The corresponding labels are **1** for every image, since they are all **real**.\n",
    "\n",
    "We'll call the loss computed for this mini-batch `loss_real`.\n",
    "\n",
    "Pretty standard **classifier** business! We're not even *touching* the Generator at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real, _ = next(iter(circles_dl))\n",
    "x_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of real samples are ones\n",
    "batch_size = x_real.size(0)\n",
    "y_real = torch.ones(batch_size, 1)\n",
    "yhat_real = discriminator(x_real)\n",
    "yhat_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "loss_real = loss_fn(yhat_real, y_real)\n",
    "loss_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch of Fake Images\n",
    "\n",
    "Next, we use a mini-batch of **fake images** as input to the **Discriminator**, and, once again, use **binary cross-entropy** as the **loss function**.\n",
    "\n",
    "The corresponding labels are **1** for every image, since they are all **fake**.\n",
    "\n",
    "We'll call the loss computed for this mini-batch `loss_fake`.\n",
    "\n",
    "---\n",
    "\n",
    "But, **wait a minute**... we **don't have a data loader for fake images**.\n",
    "\n",
    "That's when the **Generator** comes in - we use it *as it is* to **generate fake images** from a **mini-batch randomly sampled from the latent space**.\n",
    "\n",
    "We're **using the Generator** for our **fake image needs**, but it is **NOT** part of the training yet!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/disc_train2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "input_z = latent_sample(batch_size, z_size, 'normal')\n",
    "x_fake = generator(input_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels of fake samples are zeros\n",
    "y_fake = torch.zeros(batch_size, 1)\n",
    "yhat_fake = discriminator(x_fake)\n",
    "yhat_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fake = loss_fn(yhat_fake, y_fake)\n",
    "loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we computed **two losses** using the **same model (Discriminator)**.\n",
    "\n",
    "The pressing question now is: How to use the losses to actually **update the parameters**?\n",
    "\n",
    "There are two losses, so we either:\n",
    "- use each loss to perform an update, hence, **two updates**\n",
    "- add both losses together to a `loss_total`, so there's only a **single update**\n",
    "\n",
    "Honestly, this is **confusing** for GAN beginners! You'll find **both** implementations in books and blog posts, and, as many things in our field, **there's no right or wrong** answer here.\n",
    "\n",
    "What's **important** is to get the **code right**:\n",
    "- if you're going for two updates, make sure to **zero the gradients** after updating the parameters the first time.\n",
    "- if you're going for a single update, make sure you call the **`backward()`** method on the **total loss**.\n",
    "\n",
    "We may also `detach()` the generated images from the computation graph to avoid useless gradient computation (see the Generate Fake Data section in red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/discriminator_step.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Discriminator Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to make sure that your **Discriminator** is **up to the task**. We can't have a *weak* Discriminator, otherwise it will be easily fooled by the Generator, and the generated samples will be bad.\n",
    "\n",
    "One easy way to perform a quick-check on the Discriminator's abilities, is to train it separately using real and fake (at this point, noise!) images. It should be able to easily classify them correctly after little training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Training\n",
    "fig = plot_distrib_real_vs_fake(yhat_real, yhat_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An untrained Discriminator cannot distinguish real circles from noise! No surprises...\n",
    "\n",
    "Let's train it for three epochs and see what happens...\n",
    "\n",
    "BTW: we're using **two updates** to train the Discriminator. I encourage you to inspect it thoroughly to make sure you're comfortable with full sequence of steps, from the forward passes, to the parameters updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "discriminator.to(device)\n",
    "generator.to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optim_d = torch.optim.Adam(discriminator.parameters(), 0.0003)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses_d = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    batch_losses_d = []\n",
    "    \n",
    "    for i, (x_real, _) in enumerate(circles_dl):\n",
    "        batch_size = x_real.size(0)\n",
    "        x_real = x_real.to(device)\n",
    "        \n",
    "        # trains discriminator\n",
    "        discriminator.train()\n",
    "        optim_d.zero_grad()\n",
    "        # real data\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_real = discriminator(x_real)\n",
    "        # Step 2 - Computes the loss\n",
    "        y_real = torch.ones(batch_size, 1, device=device)\n",
    "        loss_real = loss_fn(yhat_real, y_real)\n",
    "\n",
    "        # Step 3 - Computes gradients\n",
    "        loss_real.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_d.step()\n",
    "\n",
    "        # fake data\n",
    "        optim_d.zero_grad()\n",
    "        input_z = latent_sample(batch_size, z_size, 'uniform').to(device)\n",
    "        x_fake = generator(input_z).detach()\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_fake = discriminator(x_fake)\n",
    "        # Step 2 - Computes the loss\n",
    "        y_fake = torch.zeros(batch_size, 1, device=device)\n",
    "        loss_fake = loss_fn(yhat_fake, y_fake)\n",
    "        \n",
    "        # Step 3 - Computes gradients\n",
    "        loss_fake.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_d.step()\n",
    "\n",
    "        loss_total = loss_real + loss_fake\n",
    "\n",
    "        batch_losses_d.append(loss_total.data.item())\n",
    "                \n",
    "    # Average over batches\n",
    "    train_losses_d.append(np.array(batch_losses_d).mean())\n",
    "\n",
    "    print(f'Epoch {epoch:03d} | Loss >> {train_losses_d[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "fig = plot_distrib_real_vs_fake(yhat_real, yhat_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect separation! Our Discriminator is definitely up to the task!\n",
    "\n",
    "---\n",
    "\n",
    "Although we **want** our Discriminator to be **strong**, we cannot make it **too strong** (compared to the Generator, that is), otherwise we may end up with **vanishing gradients** and a model that doesn't learn.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch of Pretend Data\n",
    "\n",
    "For the Generator, we use *another* mini-batch of **fake data**, but we'll **pretend they are real**, thus using **1** as their labels.\n",
    "\n",
    "We'll feed this mini-batch to the Discriminator and compute the loss using BCE as usual, calling it `loss_pretend`.\n",
    "\n",
    "Just like before, we use the **Generator** to generate fake images from a mini-batch randomly sampled from the latent space. But, **we're not detaching** the fake images since we **need gradients for the Generator**, which we're training at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/gen_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters\n",
    "\n",
    "Since we only have one loss to handle, this sequence of steps is simpler, but there are still **two differences from a typical training loop**:\n",
    "- we're not retrieving images from a data loader, but generating them using the model we're training (Generator)\n",
    "- we're computing the loss using a model we're NOT training (Discriminator), and backpropagating the gradients all the way up to the model we're interested in (Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/generator_step.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for the full GAN will include the three losses above: `loss_real`, `loss_fake`, and `loss_pretend`.\n",
    "\n",
    "Remember that we need to **train Discriminator and Generator alternately**, so how do we do that?\n",
    "\n",
    "It's actually simple: we assign **two optimizers**, one for the **Discriminator** part of the GAN model (`gan.discriminator`), the other for the **Generator** part of the GAN model (`gan.generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "reset_parameters(gan)\n",
    "\n",
    "gan.to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "# separate optimizers to train them separately\n",
    "optim_d = torch.optim.SGD(gan.discriminator.parameters(), 0.0002)\n",
    "optim_g = torch.optim.Adam(gan.generator.parameters(), 0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice a small detail, we're using **different optimizers**, namely, **Adam** and **SGD**:\n",
    "\n",
    "- Adam for the Generator\n",
    "- SGD for the Discriminator\n",
    "\n",
    "You're welcome to try using Adam for both, and see the results. In this particular problem, training wasn't going so well. As I said at the start, GANs **may be *finicky*** and difficult to train.\n",
    "\n",
    "The Adam optimizer is known for its **speed**, it quickly converges to a minimum. It could be that, by using Adam in the Discriminator, it gets too strong too quickly, and the Generator does not stand a chance.\n",
    "\n",
    "The SGD optimizer, on the other hand, may not be as quick as Adam, but it still goes in the right direction. Apparently, this buys the Generator some time to get better at generating images.\n",
    "\n",
    "This is actually tip #9 from Soumith Chintala's [GAN Hacks](https://github.com/soumith/ganhacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "train_losses_d = []\n",
    "train_losses_d_real = []\n",
    "train_losses_d_fake = []\n",
    "train_losses_g = []\n",
    "\n",
    "epoch_samples = []\n",
    "fixed_z = latent_sample(5, z_size, 'normal').to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    batch_losses_d = []\n",
    "    batch_losses_d_real = []\n",
    "    batch_losses_d_fake = []\n",
    "    batch_losses_g = []\n",
    "    \n",
    "    for i, (x_real, _) in enumerate(circles_dl):\n",
    "        batch_size = x_real.size(0)\n",
    "        \n",
    "        #####################################################\n",
    "        # Discriminator\n",
    "        #####################################################\n",
    "        gan.discriminator.train()\n",
    "        optim_d.zero_grad()\n",
    "        \n",
    "        #####################################################\n",
    "        # Real data\n",
    "        #####################################################\n",
    "        x_real = x_real.to(device)\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_real = gan.discriminator(x_real)\n",
    "\n",
    "        # Step 2 - Computes the loss\n",
    "        y_real = torch.ones(batch_size, 1, device=device)\n",
    "        loss_real = loss_fn(yhat_real, y_real)\n",
    "        \n",
    "        # Step 3 - Computes gradients\n",
    "        loss_real.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_d.step()\n",
    "\n",
    "        optim_d.zero_grad()\n",
    "        \n",
    "        #####################################################\n",
    "        # Fake data\n",
    "        #####################################################\n",
    "        input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "        x_fake = gan.generator(input_z).detach()        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_fake = gan.discriminator(x_fake)\n",
    "        \n",
    "        # Step 2 - Computes the loss\n",
    "        y_fake = torch.zeros(batch_size, 1, device=device)\n",
    "        loss_fake = loss_fn(yhat_fake, y_fake)\n",
    "\n",
    "        # Step 3 - Computes gradients\n",
    "        loss_fake.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_d.step()\n",
    "\n",
    "        loss_total = loss_real + loss_fake\n",
    "        batch_losses_d_real.append(loss_real.data.item())\n",
    "        batch_losses_d_fake.append(loss_fake.data.item())\n",
    "        batch_losses_d.append(loss_total.data.item()/2)\n",
    "        \n",
    "        #####################################################\n",
    "        # Generator\n",
    "        #####################################################\n",
    "        gan.generator.train()\n",
    "        optim_g.zero_grad()\n",
    "        \n",
    "        #####################################################\n",
    "        # Fake/Pretend data\n",
    "        #####################################################\n",
    "        input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "        x_pretend = gan.generator(input_z) # NO DETACH HERE!\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_pretend = gan.discriminator(x_pretend)\n",
    "\n",
    "        # Step 2 - Computes the loss\n",
    "        y_pretend = torch.ones(batch_size, 1, device=device)\n",
    "        loss_pretend = loss_fn(yhat_pretend, y_pretend)\n",
    "        \n",
    "        # Step 3 - Computes gradients\n",
    "        loss_pretend.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_g.step()\n",
    "        \n",
    "        batch_losses_g.append(loss_pretend.data.item())\n",
    "        \n",
    "    # Generates some samples from a fixed point in latent space\n",
    "    # to visualize progress\n",
    "    x_vis = gan.generator(fixed_z).detach().cpu().numpy()\n",
    "    epoch_samples.append(x_vis)\n",
    "\n",
    "    # Average over batches\n",
    "    train_losses_d.append(np.array(batch_losses_d).mean())\n",
    "    train_losses_d_real.append(np.array(batch_losses_d_real).mean())\n",
    "    train_losses_d_fake.append(np.array(batch_losses_d_fake).mean())\n",
    "    train_losses_g.append(np.array(batch_losses_g).mean())\n",
    "\n",
    "    print(f'Epoch {epoch:03d} | Loss G/D >> {train_losses_g[-1]:.4f}/{train_losses_d[-1]:.4f} '\n",
    "          f'Real/Fake >> {train_losses_d_real[-1]:.4f}/{train_losses_d_fake[-1]:.4f}'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_losses(train_losses_g, train_losses_d, train_losses_d_fake, train_losses_d_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you like the losses above? Doesn't look very good, right?\n",
    "\n",
    "But that's actually not so bad - remember that the two networks, the Discriminator and the Generator, are in a fierce competition. This means that the **Generator may be producing better images and its loss is still going up** just because the Discriminator got even better! As long as the losses do not oscillate like crazy, your model is probably doing fine.\n",
    "\n",
    "Also, remember that the Discriminator may learn to pick on *very faint signals* from the fake images, some details that you and I, as humans, cannot even perceive.\n",
    "\n",
    "So, always check the generated images yourself!\n",
    "\n",
    "First, let's take a look at 5 points sampled from the latent space, and the corresponding generated images as training goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 5, 10, 50, 100, 200]\n",
    "fig = plot_evolution(epoch_samples, selected_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, right? By the 100th epoch, it was already producing circles!\n",
    "\n",
    "Let's generate some more random fake images, and plot them next to some real images (the circles we generated at the start using Matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = comparison(gan.generator, z_size, circles_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple GAN generated many fine circles, and some obviously fake ones.\n",
    "\n",
    "How is our Discriminator doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_distrib_real_vs_fake(yhat_real, yhat_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the Discriminator can easily tell apart the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "- Uninformative Loss: Increasing losses are not necessarily bad, as both Discriminator and Generator are competing and improving over time, thus rendering comparison across epochs meaningless.\n",
    "\n",
    "- Oscillating Loss: Ideally, loss stabilizes or gradually increases or decreases, without oscillating wildly. Plain-vanilla GANs are especially prone to this behavior.\n",
    "\n",
    "- Mode Collapse: generator finds a \"cheat code\" that's able to fool the discriminator without generating meaningful images. It may find a single observation (the mode) that consistently fools the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Improving your GAN using Wasserstein distance (WGAN and WGAN-GP)\n",
    "\n",
    "The challenges above can be tackled by changing the loss function: instead of binary cross-entropy, we should use **Wasserstein Distance** (aka [Earth mover's distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)).\n",
    "\n",
    "We're not delving into the theoretical framework that supports this choice. Instead, we're focusing on its implementation details, and the interpretation of its results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation behind using Wasserstein Loss instead of binary cross-entropy is two-fold:\n",
    "\n",
    "- the loss is more informative, since it correlates with improvement in Generator's ability to generate quality images.\n",
    "- the loss is more stable (less oscillations)\n",
    "\n",
    "It requires the **labels** to be **changed** accordingly:\n",
    "- `-1` for negative class (fake images)\n",
    "- `+1` for positive class (real images)\n",
    "\n",
    "The implementation is actually quite simple: it's the negative average of the product of the **actual label ($y$)** and the **Discriminator's output ($\\hat{y}$)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Wasserstein Loss} = -\\frac{1}{N}\\sum_i{\\left(\\hat{y_i} \\ y_i\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Wasserstein Loss} = \\begin{cases}\n",
    "-\\frac{1}{N}\\sum_i\\hat{y_i}, \\text{for real data } (y_i = +1)\n",
    "\\\\\n",
    "+\\frac{1}{N}\\sum_i\\hat{y_i}, \\text{for fake data } (y_i = -1)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_fake.mean() - yhat_real.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're using Wasserstein loss, we also need to **remove Discriminator's sigmoid activation**, after all, it was producing probabilities between 0 and 1, and now the **negative class isn't `0` anymore, but `-1`**.\n",
    "\n",
    "For this reason, the Discriminator is now known as **Critic**, and its output is a **score**, not a probability anymore.\n",
    "\n",
    "This also means that **losses may be negative**, which may look weird at first sight, but it makes sense if you take your time to look at the possible combinations of labels and outputs from the Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large (y_i = +1) \\text{ and } (\\hat{y_i} > 0) \\implies \\text{loss} < 0\n",
    "\\\\\n",
    "\\Large (y_i = -1) \\text{ and } (\\hat{y_i} < 0) \\implies \\text{loss} < 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large (y_i = +1) \\text{ and } (\\hat{y_i} < 0) \\implies \\text{loss} > 0\n",
    "\\\\\n",
    "\\Large (y_i = -1) \\text{ and } (\\hat{y_i} > 0) \\implies \\text{loss} > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**IMPORTANT: If our model is doing well, its loss will be negative!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "def wasserstein_loss(\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    size_average: Optional[bool] = None,\n",
    "    reduce: Optional[bool] = None,\n",
    "    reduction: str = \"mean\",\n",
    ") -> torch.Tensor:\n",
    "    if torch.overrides.has_torch_function_variadic(input, target):\n",
    "        return handle_torch_function(\n",
    "            wasserstein_loss, (input, target), input, target, size_average=size_average, reduce=reduce, reduction=reduction\n",
    "        )\n",
    "    if not (target.size() == input.size()):\n",
    "        warnings.warn(\n",
    "            \"Using a target size ({}) that is different to the input size ({}). \"\n",
    "            \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "            \"Please ensure they have the same size.\".format(target.size(), input.size()),\n",
    "            stacklevel=2,\n",
    "        )\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = nn._reduction.legacy_get_string(size_average, reduce)\n",
    "\n",
    "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
    "    \n",
    "    distance = -(expanded_input * expanded_target)\n",
    "    if reduction == 'none':\n",
    "        out = distance\n",
    "    elif reduction == 'mean':\n",
    "        out = distance.mean()\n",
    "    elif reduction == 'sum':\n",
    "        out = distance.sum()\n",
    "    else:\n",
    "        raise ValueError(\"{} is not a valid value for reduction\".format(reduction))\n",
    "        \n",
    "    return out\n",
    "\n",
    "class WassersteinLoss(nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(WassersteinLoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return wasserstein_loss(input, target, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator (Wasserstein) aka Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the Discriminator/Critic does not have a sigmoid activation anymore, and it outputs a **score** in the range (-inf, +inf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorW(nn.Module):\n",
    "    def __init__(self, input_shape, base_model):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.base_model = base_model\n",
    "\n",
    "        output_size = self._get_output_size()\n",
    "        # appends the \"lin_classifier\" linear layer to map from \"output_size\" \n",
    "        # given by the base model to the output of a binary classifier\n",
    "        output_size = self._get_output_size()\n",
    "        self.lin_classifier = nn.Linear(output_size, 1)\n",
    "        # Removing the sigmoid activation\n",
    "        # self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def _get_output_size(self):\n",
    "        # builds a dummy batch containing one dummy tensor\n",
    "        # full of zeroes with the same shape as the inputs\n",
    "        device = next(self.base_model.parameters()).device.type\n",
    "        dummy = torch.zeros(1, *self.input_shape, device=device)\n",
    "        # sends the dummy batch through the base model to get \n",
    "        # the output size produced by it\n",
    "        size = self.base_model(dummy).size(1)\n",
    "        return size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forwards the input through the base model and then the \"lin_latent\" layer \n",
    "        # to get the representation (z)\n",
    "        base_out = self.base_model(x)\n",
    "        logits = self.lin_classifier(base_out)\n",
    "        return logits\n",
    "        # Removing the sigmoid activation\n",
    "        # out = self.activation(logits)\n",
    "        # return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator's base model is **almost** the same as before - can you spot the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "z_size = 20\n",
    "n_filters = 32\n",
    "in_channels = 1\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "## No batch norm, no sigmoid!\n",
    "disc_base = nn.Sequential(\n",
    "    # in_channels@28x28 -> 32@28x28\n",
    "    nn.Conv2d(in_channels, n_filters, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters),\n",
    "    nn.LeakyReLU(0.2),\n",
    "            \n",
    "    # 32@28x28 -> 64@14x14\n",
    "    nn.Conv2d(n_filters, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@14x14 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 3136\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "discriminatorW = DiscriminatorW(input_shape, disc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER: We replaced `BatchNorm2d` by `InstanceNorm2d`.**\n",
    "\n",
    "We'll get back to that soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Once again, we replace `BatchNorm2d` by `InstanceNorm2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatorW = nn.Sequential(\n",
    "    # unflatten to generate z_size as pixel\n",
    "    nn.Unflatten(1, (z_size, 1, 1)),\n",
    "    \n",
    "    # z_size@1x1 -> 64@4x4\n",
    "    nn.ConvTranspose2d(z_size, n_filters*2, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@4x4 -> 64@7x7\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@7x7 -> 32@14x14\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 32@14x14 -> in_channels@28x28\n",
    "    nn.ConvTranspose2d(n_filters, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GAN that uses a Wasserstein loss is known as WGAN, and it was introduced by Arjovsky, M. et al. in the paper [Wasserstein GAN](https://arxiv.org/abs/1701.07875)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganW = GAN(generatorW, discriminatorW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, changing the loss is not enough. There are more things one needs to take into account to successfully train a WGAN.\n",
    "\n",
    "a) Replace `BatchNorm` by `InstanceNorm` (that's actually for WGAN-GP, not the original WGAN, but let's jump directly to it).\n",
    "\n",
    "b) Removing the sigmoid activation from the Discriminator/Critic made it **unconstrained**, since its scores can go from `-inf` to `+inf`. In order to be able to train a WGAN, the Discriminator/Critic needs to **constrained** once again, forcing it to be a so-called *1-Lipschitz continuous function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) InstanceNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with `BatchNorm` is that it creates correlations between data points in the same batch, and it negatively impacts the effectiveness of the **gradient penalty** (we'll get to that soon).\n",
    "\n",
    "So, we choose `InstanceNorm` instead. It **normalizes individual channels** in sample images (NCHW). For example, in 3-channel 16x16 images (3@16x16), it will compute three pairs of statistics (mean and standard deviation), one pair a channel.\n",
    "\n",
    "`InstanceNorm` is quite similar to `LayerNorm`, but the latter is often used in **sequence problems**, where there are three-dimensions only (mini-batch, sequence length, number of features). One can use `LayerNorm` to get the *same result* as `InstanceNorm` if one uses the last two dimensions as argument.\n",
    "\n",
    "Hopefully the diagram and code below makes it more clear. We're using two batches, each batch containing only 2 samples (N=2), each sample being a 3-channel (C=3) 16x16 (H=16, W=16) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (4, 3, 16, 16) # N C H W\n",
    "torch.manual_seed(34)\n",
    "inputs = torch.randn(*input_shape)\n",
    "batches = inputs.split(2)\n",
    "batches[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/images/instance_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_normalized = nn.InstanceNorm2d(3)(batches[0])\n",
    "inst_normalized.mean(axis=[2, 3]), inst_normalized.var(axis=[2, 3], unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two samples in a batch, three channels in a sample, so there are 6 pairs of statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 1-Lipschitz Continuous Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a mouthful, right? Let's see, and visualize, what a function has to look like to be a *1-Lipschitz Continuous Function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\frac{\\lvert f(x_1) - f(x_2) \\rvert}{\\lvert x_1 - x_2 \\rvert} \\leq k \\implies \\frac{\\lvert\\Delta{y}\\rvert}{\\lvert\\Delta{x}\\rvert} \\leq k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every pair of $x$ values, we divide the (absolute) change in $y$ by the (absolute) change in $x$.\n",
    "\n",
    "If **all** results are less than or equal to a given $k$, the function is said to be *k-Lipschitz*.\n",
    "\n",
    "Let's look at some examples for ***k*=1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 101)\n",
    "fs = [make_f(.8, 1.), make_f(1.3, 0.), lambda x: np.sin(x+1), lambda x: np.cos(2*x), lambda x: (x-2)**2/4, lambda x: (x-2)**2/4]\n",
    "x0s = [0, 0, -1, -1, 2, 3]\n",
    "titles = [r'$f(x)=0.8x+1$', r'$f(x)=1.3x$', r'$f(x)=sin(x+1)$', r'$f(x)=cos(2x)$', r'$f(x)=\\frac{1}{4}(x-2)^2$', r'$f(x)=\\frac{1}{4}(x-2)^2$']\n",
    "k = 1\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axs = axs.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    lipschitz(x, fs[i], x0s[i], titles[i], k, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots above, only the *first* and *third* plots of the first row are 1-Lipschitz.\n",
    "\n",
    "They are the only ones where **the function lies completely inside the blue region**, regardless of the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-Lipschitz Discriminator/Critic\n",
    "\n",
    "In our case, the **function** is the whole neural network used as **Discriminator/Critic**, and D($x$) is the **score** it outputs.\n",
    "\n",
    "The inputs ($x$) are, in our case, the **images**, both fake and real, and $x_1 - x_2$ is the average absolute difference between two images, pixelwise.\n",
    "\n",
    "$$\n",
    "\\Large \\frac{\\lvert D(x_1) - D(x_2) \\rvert}{\\lvert x_1 - x_2 \\rvert} \\leq 1\n",
    "$$\n",
    "\n",
    "So, for the Discriminator to be a k-Lipschitz Continous Function, the **norm of the gradients of its scores w.r.t to the pixels in the images must be at most *k***.\n",
    "\n",
    "For WGANs to work, the authors of the paper show it is required that ***k*=1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How To Enforce the 1-Lipschitz-ness?\n",
    "\n",
    "1. **Weight Clipping** using a small range [-0.01, 0.01] as proposed by the authors of the WGAN paper.\n",
    " - *“Weight clipping is a clearly terrible way to enforce a Lipschitz constraint.”*\n",
    " - It weakens the Critic.\n",
    " - We need a strong Critic to push the Generator to learn how to produce good samples.\n",
    " - It can lead to exploding or vanishing gradients.\n",
    " \n",
    "\n",
    "2. **Gradient Penalty (GP)**, including a new term in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of including a gradient penalty term to the loss was introduced in the paper [\"Improved Training of Wasserstein GANs\"](https://arxiv.org/abs/1704.00028) by Gulrajani, I. et al.\n",
    "\n",
    "We already know that, for the Discriminator/Critic to be a 1-Lipschitz function, the norm of the gradients of its scores w.r.t. the input images must be at most one.\n",
    "\n",
    "So, why not include a new term in the loss that **penalizes** the model if the **norm of the gradients deviates from 1**?\n",
    "\n",
    "This way, minimizing the loss, that is, *training* the model, will naturally push the model to conform to the 1-Lipschitz constraint.\n",
    "\n",
    "This new term is defined as the **mean squared error** between the **norm of the gradients** and the **desired norm, one**.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\text{GP} = \\lambda \\frac{1}{N}\\sum_{i=0}^N \\left( \\| \\nabla_i \\|_2-1 \\right)^2\n",
    "$$\n",
    "\n",
    "On top of that, it also includes a **multiplying factor** (`lambda_gp`), usually 10.\n",
    "\n",
    "Now the question is, which images should one use to compute the norm of the gradients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Gradients\n",
    "\n",
    "We have some real images, and an infinite supply of fake images (after all, we can keep the Generator producing them as long as we want).\n",
    "\n",
    "We cannot obviously use the *whole latent space*, so we settle for a handful of points only.\n",
    "\n",
    "Better yet, we settle for some **images in between real and fake**, that is, **interpolated**.\n",
    "\n",
    "It goes like this:\n",
    "1. For each pair of fake and real samples in a given batch, we **choose a random number (`alpha`) between 0 and 1**\n",
    "2. Then we **interpolate between the two images**, pixelwise, using `alpha`\n",
    "3. We feed the interpolated image to the Critic and **get the corresponding score**\n",
    "4. Next, we **compute the gradients of the score w.r.t to the interpolated image** (each pixel has a gradient)\n",
    "5. We take the **l2-norm of the gradients**, so there's one value for each interpolated image\n",
    "6. Then we **compute the MSE**, assuming we want every norm to be as close to 1 as possible, and **apply the multiplying factor `lambda_gp`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over the steps above for a mini-batch of five real, and five fake, images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "latent = latent_sample(5, z_size, 'uniform').to(device)\n",
    "fake_samples = generatorW.to(device)(latent).detach().cpu()\n",
    "real_samples = next(iter(circles_dl))[0][:5]\n",
    "fake_samples.shape, real_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choose a random number (`alpha`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.rand(5, 1, 1, 1, requires_grad=True)\n",
    "alpha.shape, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Interpolate between fake and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "interpolated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get Critic's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_interpolated = discriminatorW.to(device)(interpolated.to(device))\n",
    "score_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_interpolated(z_size, ganW, circles_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute the gradients of the score w.r.t. interpolated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "gradients = torch_grad(\n",
    "    outputs=score_interpolated, inputs=interpolated,\n",
    "    grad_outputs=torch.ones(score_interpolated.size(), device=device),\n",
    "    create_graph=True, retain_graph=True\n",
    ")[0]\n",
    "\n",
    "gradients.shape, gradients.min(), gradients.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Take l2-norm of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually\n",
    "norms = (gradients**2).sum(dim=[1, 2, 3]).sqrt()\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using norm method\n",
    "grad_norm = gradients.view(5, -1).norm(2, dim=1)\n",
    "grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pixel_gradients(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compute MSE and apply lambda factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_gp = 10\n",
    "penalty = lambda_gp * ((1-grad_norm)**2).mean()\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.tensor([[.45, .5], \n",
    "                  [.75, -.4], \n",
    "                  [-1.55, -.75]]).view(3, 2) # N, L, H\n",
    "fig = grad_norms(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, x_real, x_fake, lambda_gp=10):\n",
    "    batch_size = x_real.size(0)\n",
    "    # random alpha between 0 and 1 for interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, requires_grad=True).type_as(x_real)\n",
    "\n",
    "    # interpolated samples\n",
    "    interpolated = alpha * x_real + (1 - alpha) * x_fake\n",
    "    # critic score for each sample\n",
    "    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "    # how much the score changes if the sample is modified?\n",
    "    # large gradients mean the critic is oversensitive\n",
    "    gradients = torch_grad(\n",
    "        outputs=prob_interpolated, inputs=interpolated,\n",
    "        grad_outputs=torch.ones(prob_interpolated.size()).type_as(x_real),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0].view(batch_size, -1)\n",
    "\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "    return lambda_gp * ((grad_norm-1)**2).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (WGAN-GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop of a WGAN-GP is **almost** the same as our former training loops, except for a few differences:\n",
    "\n",
    "- before, we were concerned about keeping Discriminator and Generator balanced, but WGANs are different - we **should** train the Critic/Discriminator to convergence, so the gradients for the Generator are more accurate - so we **train the Critic SEVERAL times** (usually 5 iterations) before **training the Generator once**.\n",
    "- instead of using *two updates* (one for real loss, one for fake loss) for the Critic/Discriminator, we're now using a **single update** using the **total loss** (real, fake, and GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganW.to(device)\n",
    "\n",
    "loss_fn = WassersteinLoss()\n",
    "\n",
    "optim_d = torch.optim.SGD(ganW.discriminator.parameters(), 0.0002)\n",
    "optim_g = torch.optim.Adam(ganW.generator.parameters(), 0.0002)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses_d = []\n",
    "train_losses_d_real = []\n",
    "train_losses_d_fake = []\n",
    "train_losses_d_penalty = []\n",
    "train_losses_g = []\n",
    "\n",
    "critic_iterations = 5\n",
    "\n",
    "fixed_z = latent_sample(5, z_size, 'normal').to(device)\n",
    "epoch_samples = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    batch_losses_d = []\n",
    "    batch_losses_d_real = []\n",
    "    batch_losses_d_fake = []\n",
    "    batch_losses_d_penalty = []\n",
    "    batch_losses_g = []\n",
    "    \n",
    "    for i, (x_real, _) in enumerate(circles_dl):\n",
    "        batch_size = x_real.size(0)\n",
    "        \n",
    "        for i in range(critic_iterations):\n",
    "            #####################################################\n",
    "            # Critic\n",
    "            #####################################################\n",
    "            ganW.discriminator.train()\n",
    "            optim_d.zero_grad()\n",
    "            \n",
    "            #####################################################\n",
    "            # Real data\n",
    "            #####################################################\n",
    "            x_real = x_real.to(device)\n",
    "            y_real = torch.ones(batch_size, 1, device=device)\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat_real = ganW.discriminator(x_real)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss_real = loss_fn(yhat_real, y_real)\n",
    "\n",
    "            #####################################################\n",
    "            # Fake data\n",
    "            #####################################################\n",
    "            input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "            x_fake = ganW.generator(input_z).detach()\n",
    "            y_fake = -torch.ones(batch_size, 1, device=device)\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat_fake = ganW.discriminator(x_fake)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss_fake = loss_fn(yhat_fake, y_fake)\n",
    "\n",
    "            #####################################################\n",
    "            # Gradient Penalty\n",
    "            #####################################################            \n",
    "            loss_penalty = gradient_penalty(ganW.discriminator, x_real.data, x_fake.data)\n",
    "\n",
    "            loss_total = loss_real + loss_fake + loss_penalty\n",
    "            # Step 3 - Computes gradients\n",
    "            loss_total.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            optim_d.step()\n",
    "        \n",
    "        batch_losses_d_real.append(loss_real.data.item())\n",
    "        batch_losses_d_fake.append(loss_fake.data.item())\n",
    "        batch_losses_d_penalty.append(loss_penalty.data.item())\n",
    "        batch_losses_d.append((loss_real + loss_fake).data.item()/2)\n",
    "        \n",
    "        #####################################################\n",
    "        # Generator\n",
    "        #####################################################\n",
    "        ganW.generator.train()\n",
    "        optim_g.zero_grad()\n",
    "        \n",
    "        #####################################################\n",
    "        # Fake/Pretend data\n",
    "        #####################################################        \n",
    "        input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "        y_pretend = torch.ones(batch_size, 1, device=device)\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_pretend = ganW(input_z)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss_pretend = loss_fn(yhat_pretend, y_pretend)\n",
    "        \n",
    "        # Step 3 - Computes gradients\n",
    "        loss_pretend.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_g.step()\n",
    "        \n",
    "        batch_losses_g.append(loss_pretend.data.item())\n",
    "\n",
    "    # Average over batches\n",
    "    train_losses_d.append(np.array(batch_losses_d).mean())\n",
    "    train_losses_d_real.append(np.array(batch_losses_d_real).mean())\n",
    "    train_losses_d_fake.append(np.array(batch_losses_d_fake).mean())\n",
    "    train_losses_d_penalty.append(np.array(batch_losses_d_penalty).mean())\n",
    "    train_losses_g.append(np.array(batch_losses_g).mean())\n",
    "    \n",
    "    # Generates some samples from a fixed point in latent space\n",
    "    # to visualize progress\n",
    "    x_vis = ganW.generator(fixed_z).detach().cpu().numpy()\n",
    "    epoch_samples.append(x_vis)    \n",
    "\n",
    "    print(f'Epoch {epoch:03d} | Loss G/D >> {train_losses_g[-1]:.4f}/{train_losses_d[-1]:.4f} '\n",
    "          f'Real/Fake/Penalty >> {train_losses_d_real[-1]:.4f}/{train_losses_d_fake[-1]:.4f}/{train_losses_d_penalty[-1]:.4f}'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(train_losses_g, train_losses_d, train_losses_d_fake, train_losses_d_real, train_losses_d_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the losses are more stable now. The gradient penalty is quite close to zero, meaning the Critic/Discriminator was successfully constrained as a 1-Lipschitz function.\n",
    "\n",
    "How about the actual generated images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 5, 10, 20, 50]\n",
    "fig = plot_evolution(epoch_samples, selected_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = comparison(ganW.generator, z_size, circles_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_distrib_real_vs_fake(yhat_real, yhat_fake, is_critic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some intersection in the histogram above, so the Critic/Discriminator sometimes can't tell the difference between a fake and a real image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up: GANs N' Roses\n",
    "\n",
    "It's time to generate some synthetic roses!\n",
    "\n",
    "We've finally arrived at the part that inspired the name of this workshop :-)\n",
    "\n",
    "We'll be using a subset of [VGG's 102 Category Flower Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html).\n",
    "\n",
    "It's kinda frustrating that the actual labels are not there, and I can't identify many flowers myself, but luckily someone else ran into this problem and figured out the labels: https://github.com/bdevnani3/oxfordflowers102-label-name-mapping/blob/main/mapping.json\n",
    "\n",
    "Assuming those labels are correct, I got category 74 (rose), and tried to split it into different colors (orange, red, rose, white, and yellow), but we're not actually using these sub-labels in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz -o 102flowers.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "def extract_flowers(dataset, fnames, outdir):\n",
    "    df = pd.read_csv(fnames)\n",
    "    \n",
    "    for category in df.category.unique():\n",
    "        if not os.path.exists(f'{outdir}/{category}'):\n",
    "            os.makedirs(f'{outdir}/{category}')\n",
    "\n",
    "    t = tarfile.open(dataset, 'r')\n",
    "    \n",
    "    for member in t.getmembers():\n",
    "        match = (df.fname == member.name[4:])\n",
    "        if match.any():\n",
    "            fname, category = df.loc[match.idxmax()]\n",
    "            t._extract_member(member, f\"{outdir}/{category}/{fname}\", set_attrs=True, numeric_owner=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://raw.githubusercontent.com/dvgodoy/GANsNRoses_ODSC_Europe2022/main/gans.py --output gans.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'flowers/roses'\n",
    "dataset = '102flowers.tgz'\n",
    "fnames = 'roses.csv'\n",
    "extract_flowers(dataset, fnames, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import CenterCrop, Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "flowers_ds = ImageFolder(root='flowers/roses', transform=Compose([CenterCrop(500), Resize(28), ToTensor(), Normalize(mean=(0.5), std=(0.5))]))\n",
    "flowers_dl = DataLoader(flowers_ds, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = preview(flowers_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP\n",
    "\n",
    "We're using the same models as before, but the size of the latent space is 40 now, since our images are more complex than circles.\n",
    "\n",
    "Moreover, we're using `Dropout` layers in the Generator, that's tip #17 from Soumith Chintala's [GAN Hacks](https://github.com/soumith/ganhacks).\n",
    "\n",
    "Adding dropout actually **increased variability** of the generated samples **by a lot**! I encourage you to try removing those layers (or setting the probability to zero), retrain the model, and see the difference yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "z_size = 40\n",
    "n_filters = 32\n",
    "# color images = 3 channels\n",
    "in_channels = 3\n",
    "input_shape = (3, 28, 28)\n",
    "\n",
    "disc_base = nn.Sequential(\n",
    "    # in_channels@28x28 -> 32@28x28\n",
    "    nn.Conv2d(in_channels, n_filters, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters),\n",
    "    nn.LeakyReLU(0.2),\n",
    "            \n",
    "    # 32@28x28 -> 64@14x14\n",
    "    nn.Conv2d(n_filters, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@14x14 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 64@7x7\n",
    "    nn.Conv2d(n_filters*2, n_filters*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    # 64@7x7 -> 3136\n",
    "    nn.Flatten(),\n",
    ")\n",
    "\n",
    "# discriminator is the same as our former encoder, but it outputs probabilities now\n",
    "discriminatorW = DiscriminatorW(input_shape, disc_base)\n",
    "\n",
    "generatorW = nn.Sequential(\n",
    "    # unflatten to generate z_size as pixel\n",
    "    nn.Unflatten(1, (z_size, 1, 1)),\n",
    "    \n",
    "    # z_size@1x1 -> 64@4x4\n",
    "    nn.ConvTranspose2d(z_size, n_filters*2, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.Dropout2d(0.5),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@4x4 -> 64@7x7\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters*2),\n",
    "    nn.Dropout2d(0.5),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 64@7x7 -> 32@14x14\n",
    "    nn.ConvTranspose2d(n_filters*2, n_filters, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.InstanceNorm2d(n_filters),\n",
    "    nn.Dropout2d(0.5),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    # 32@14x14 -> in_channels@28x28\n",
    "    nn.ConvTranspose2d(n_filters, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "ganW = GAN(generatorW, discriminatorW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganW.to(device)\n",
    "\n",
    "loss_fn = WassersteinLoss()\n",
    "\n",
    "optim_d = torch.optim.SGD(ganW.discriminator.parameters(), 0.0003)\n",
    "optim_g = torch.optim.Adam(ganW.generator.parameters(), 0.0003)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "train_losses_d = []\n",
    "train_losses_d_real = []\n",
    "train_losses_d_fake = []\n",
    "train_losses_d_penalty = []\n",
    "train_losses_g = []\n",
    "\n",
    "critic_iterations = 5\n",
    "\n",
    "fixed_z = latent_sample(5, z_size, 'normal').to(device)\n",
    "epoch_samples = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    batch_losses_d = []\n",
    "    batch_losses_d_real = []\n",
    "    batch_losses_d_fake = []\n",
    "    batch_losses_d_penalty = []\n",
    "    batch_losses_g = []\n",
    "    \n",
    "    for i, (x_real, _) in enumerate(flowers_dl):\n",
    "        batch_size = x_real.size(0)\n",
    "        \n",
    "        for i in range(critic_iterations):\n",
    "            #####################################################\n",
    "            # Critic\n",
    "            #####################################################\n",
    "            ganW.discriminator.train()\n",
    "            optim_d.zero_grad()\n",
    "\n",
    "            #####################################################\n",
    "            # Real data\n",
    "            #####################################################            \n",
    "            x_real = x_real.to(device)\n",
    "            y_real = torch.ones(batch_size, 1, device=device)\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat_real = ganW.discriminator(x_real)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss_real = loss_fn(yhat_real, y_real)\n",
    "\n",
    "            #####################################################\n",
    "            # Fake data\n",
    "            #####################################################\n",
    "            input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "            x_fake = ganW.generator(input_z).detach()\n",
    "            y_fake = -torch.ones(batch_size, 1, device=device)\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat_fake = ganW.discriminator(x_fake)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss_fake = loss_fn(yhat_fake, y_fake)\n",
    "\n",
    "            #####################################################\n",
    "            # Gradient Penalty\n",
    "            #####################################################\n",
    "            loss_penalty = gradient_penalty(ganW.discriminator, x_real.data, x_fake.data)\n",
    "\n",
    "            loss_total = loss_real + loss_fake + loss_penalty\n",
    "            # Step 3 - Computes gradients\n",
    "            loss_total.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            optim_d.step()\n",
    "        \n",
    "        batch_losses_d_real.append(loss_real.data.item())\n",
    "        batch_losses_d_fake.append(loss_fake.data.item())\n",
    "        batch_losses_d_penalty.append(loss_penalty.data.item())\n",
    "        batch_losses_d.append((loss_real + loss_fake).data.item()/2)\n",
    "        \n",
    "        #####################################################\n",
    "        # Generator\n",
    "        #####################################################\n",
    "        ganW.generator.train()\n",
    "        optim_g.zero_grad()\n",
    "        \n",
    "        #####################################################\n",
    "        # Fake/Pretend data\n",
    "        #####################################################        \n",
    "        input_z = latent_sample(batch_size, z_size, 'normal').to(device)\n",
    "        y_pretend = torch.ones(batch_size, 1, device=device)\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat_pretend = ganW(input_z)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss_pretend = loss_fn(yhat_pretend, y_pretend)\n",
    "        \n",
    "        # Step 3 - Computes gradients\n",
    "        loss_pretend.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optim_g.step()\n",
    "        \n",
    "        batch_losses_g.append(loss_pretend.data.item())\n",
    "\n",
    "    # Average over batches\n",
    "    train_losses_d.append(np.array(batch_losses_d).mean())\n",
    "    train_losses_d_real.append(np.array(batch_losses_d_real).mean())\n",
    "    train_losses_d_fake.append(np.array(batch_losses_d_fake).mean())\n",
    "    train_losses_d_penalty.append(np.array(batch_losses_d_penalty).mean())\n",
    "    train_losses_g.append(np.array(batch_losses_g).mean())\n",
    "\n",
    "    # Generates some samples from a fixed point in latent space\n",
    "    # to visualize progress\n",
    "    x_vis = ganW.generator(fixed_z).detach().cpu().numpy()\n",
    "    epoch_samples.append(x_vis)    \n",
    "    \n",
    "    print(f'Epoch {epoch:03d} | Loss G/D >> {train_losses_g[-1]:.4f}/{train_losses_d[-1]:.4f} '\n",
    "          f'Real/Fake/Penalty >> {train_losses_d_real[-1]:.4f}/{train_losses_d_fake[-1]:.4f}/{train_losses_d_penalty[-1]:.4f}'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(train_losses_g, train_losses_d, train_losses_d_fake, train_losses_d_real, train_losses_d_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit more oscillation now, but overall still stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 5, 10, 50, 100, 200]\n",
    "fig = plot_evolution(epoch_samples, selected_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got some generated flowers indeed :-)\n",
    "\n",
    "Let's take a look at a larger selection, and compare them to the real images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = comparison(ganW.generator, z_size, flowers_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roses of different colors, and the look and feel is definitely there.\n",
    "\n",
    "Did you notice a subtle *checkered pattern* in the fake images? That's a typical artifact of the **transposed convolutions**.\n",
    "\n",
    "Let's take a closer look at some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "axs = axs.flatten()\n",
    "generated = (ganW.generator(latent_sample(10, z_size, 'normal').to(device))+1)/2\n",
    "for i in range(10):\n",
    "    show(generated[i], ax=axs[i])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe this tutorial has **most of the necessary steps** one needs go to trough in order to **learn**, in a **structured** and **incremental** way, how to **develop build Generative Adversarial Networks using PyTorch**.\n",
    "\n",
    "If you have any thoughts, comments or questions, please contact me on [LinkedIn](https://br.linkedin.com/in/dvgodoy) or [Twitter](https://twitter.com/dvgodoy).\n",
    "\n",
    "<h1><center>THANK YOU!</center></h1>\n",
    "\n",
    "<h3><center>\n",
    "    If you liked my talk and are interested in learning more about PyTorch, check my book:\n",
    "<br>\n",
    "<br>\n",
    "    Enjoy the discount for ODSC 2022 participants (33% OFF) and buy it for only USD 19.95!\n",
    "<br>\n",
    "<br>\n",
    "<a href=\"https://leanpub.com/pytorch/c/ODSC2022\">https://leanpub.com/pytorch/c/ODSC2022</a>\n",
    "<br>\n",
    "<br>\n",
    "    This coupon code is valid until the end of the conference.\n",
    "</center></h3>\n",
    "\n",
    "<h2><center>\n",
    "    For more information on the <strong>paperback</strong> edition, please my book's official website:\n",
    "<br>\n",
    "<br>\n",
    "<a href=\"https://pytorchstepbystep.com\">\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_Europe2022/main/images/book_cover.png\" width=\"60%\">\n",
    "</p>\n",
    "<br>\n",
    "    https://pytorchstepbystep.com\n",
    "</a>\n",
    "</center></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
